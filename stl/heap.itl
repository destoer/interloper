namespace std;

#attr(no_reorder)
struct HeapNode
{
    canary_start : u64;

    next : HeapNode?;
    prev : HeapNode?;

    // NOTE: inclusive Node
    blocks : u64;

    free: bool;

    canary_end : u64;
}

struct Heap
{
    free_list : HeapNode? = NULL;

    min: HeapNode?;
    max: HeapNode?;

    // NOTE: this is in block counts
    used : u64;
    capacity : u64;
    max_allocation: u64;

    total_allocation_bytes: u64;
}


constant HEAP_DEFAULT_BLOCK_COUNT : u32 = 1024;
constant HEAP_BLOCK_SIZE : usize = HeapNode.size;
constant HEAP_DEFAULT_ALLOC_SIZE : usize = HEAP_BLOCK_SIZE * HEAP_DEFAULT_BLOCK_COUNT;

constant HEAP_CANARY_START : u64 = 0xCAFE_DEAD_C0DE_BABE;
constant HEAP_CANARY_END : u64 = 0xFACE_DEAD_FEED_CAFE;

func heap_duplicate(heap: Heap@, data: const byte@, size: usize) byte@
{
    buffer := alloc_heap_panic(heap,size,sizeof_type(byte));
    memcpy(buffer,data,size);

    return buffer;
}

func usable_blocks(heap: const Heap@) u64
{
    return (heap.capacity - heap.used);
}

func heap_usage(heap: const Heap@)
{
    print("heap usage:\n");
    print("used: {}\n",heap.used * HEAP_BLOCK_SIZE);
    print("capacity: {}\n",heap.capacity * HEAP_BLOCK_SIZE);
    print("free: {}\n",usable_blocks(heap) * HEAP_BLOCK_SIZE);
    print("total allocation bytes: {}",heap.total_allocation_bytes);

    putchar('\n');
}

func print_heap(heap: const Heap@)
{
    cur := heap.free_list;

    if(!cur)
    {
        return;
    }

    // TODO: need to work on rtti so we dont need casts for this
    while(cur)
    {
        print("node {} : {x} -> {}\n",cast(byte@,cur),cur.blocks,cast(byte@,cur.next));
        cur = cur.next;
    }
}

func verify_node(heap: const Heap@,node: const HeapNode?)
{
    if(!node)
    {
        return;
    }

    node_reference := cast_ref(node);

    if(node_reference.blocks > heap.capacity)
    {
        crash_and_burn_oom("Blocks exceeded on heap node");
    }

    if(node > heap.max || node < heap.min)
    {
        crash_and_burn_oom("Node on heap is out of range");
    }

    if(node_reference.canary_start != HEAP_CANARY_START || node_reference.canary_end != HEAP_CANARY_END) 
    {
        crash_and_burn_oom("Heap canary trashed");
    }
}

func verify_heap(heap: const Heap@)
{
    cur := heap.free_list;

    if(!cur)
    {
        return;
    }

    prev : HeapNode? = NULL;

    while(cur)
    {
        // TODO: This is a interesting test case for flow typing.
        cur_reference := cast_ref(cur);

        if(cur_reference.prev != prev)
        {
            crash_and_burn_oom("Prev does not match on heap");
        }

        verify_node(heap,cur);
        prev = cur;
        cur = cur_reference.next;
    }
}

func check_heap_pointer_valid(heap: const Heap@, ptr: const byte?) bool
{
    if(ptr > heap.max)
    {
        return false;
    }

    return ptr < heap.min;
}

func free_heap(heap : Heap@, ptr : byte@?)
{
    if(!@ptr)
    {
        return;
    }

    verify_heap(heap);

    // block info is behind the buffer
    node := cast(HeapNode@, @ptr) - 1;

    if(node.blocks > heap.max_allocation)
    {
        crash_and_burn_oom("Node exceeds max allocation");
    }

    // set our ptr to null
    @ptr = NULL;

    // actually add it back to the free list
    node.free = true;
    heap.used -= node.blocks;

    next := node.next;

    // attempt to collapse upper
    // TODO: Remove jank with flow typing 
    if(next)
    {
        next_reference := cast_ref(next);
        if(next_reference.free && (node + node.blocks) == next_reference)
        {
            node.blocks += next_reference.blocks;

            // Link over next
            node.next = next_reference.next;

            // Link prev of new next
            if(node.next)
            {
                ref_next := cast_ref(node.next);
                ref_next.prev = node;
            }
        }
    }
    
    prev := node.prev;

    // Attempt to collapse lower
    if(prev)
    {
        prev_reference := cast_ref(prev);
        if(prev_reference.free && (prev_reference + prev_reference.blocks) == node)
        {
            prev_reference.blocks += node.blocks;

            // Link over node
            prev_reference.next = node.next;

            // Link back prev of node next
            if(prev_reference.next) 
            {
                ref_next := cast_ref(prev_reference.next);
                ref_next.prev = prev_reference;
            }
        }
    }
}

func request_buffer(heap : Heap@, size : usize) bool
{
    ptr := os_req_mem(size);

    if(!ptr)
    {
        return false;
    }

    add_buffer(heap,cast(byte@,ptr),size);
    return true;
}

func find_first_fit(heap: Heap@, req_blocks: usize) HeapNode?
{
    cur := heap.free_list;

    while(cur)
    {
        cur_reference := cast_ref(cur);

        if(cur_reference.blocks >= req_blocks && cur_reference.free)
        {
            return cur;
        }

        cur = cur_reference.next;
    }

    return NULL;
}

#attr(use_result)
func alloc_heap(heap : Heap@, count : usize, size : usize) byte?
{
    verify_heap(heap);

    bytes := count * size;

    req_blocks := ((bytes + HEAP_BLOCK_SIZE - 1) / HEAP_BLOCK_SIZE) + 1;


    cur := find_first_fit(heap,req_blocks);

    if(!cur)
    {
        req := max_u64(req_blocks * HEAP_BLOCK_SIZE,HEAP_DEFAULT_ALLOC_SIZE);

        // out of memory try get some more
        if(!request_buffer(heap,req))
        {
            return NULL;
        }
        
        cur = find_first_fit(heap,req_blocks);

        if(!cur)
        {
            return NULL;
        }
        
    }

    cur_reference := cast_ref(cur);

    if(cur_reference.blocks == req_blocks)
    {
        cur_reference.free = false;
    }

    else if(cur_reference.blocks >= req_blocks)
    {
        new := cur_reference;

        remain := cur_reference.blocks - req_blocks;

        // fragment the block
        cur_reference.blocks = req_blocks;
        cur_reference.free = false;

        new += cur_reference.blocks;
        new.blocks = remain;
        new.free = true;

        new.canary_start = HEAP_CANARY_START;
        new.canary_end = HEAP_CANARY_END;

        // Point new block at its neighbours
        new.prev = cur_reference;
        new.next = cur_reference.next;

        // Repoint the surrounding blocks
        if(new.next)
        {
            new_next := cast_ref(new.next);
            new_next.prev = new;
        }

        cur_reference.next = new;
    }

    // mark heap usage
    heap.used += cur_reference.blocks;
    heap.total_allocation_bytes += size;
    heap.max_allocation = max_u64(heap.max_allocation,cur_reference.blocks);
    
    return cur_reference + 1;
}

#attr(use_result)
func realloc_heap(heap : Heap@, ptr: byte@?, count: usize,size: usize) bool
{
    // get the actual data pointer
    old := @ptr;

    // first alloc just behave like alloc
    if(!old)
    {
        _ = alloc_heap(heap,count,size);
        new := alloc_heap(heap,count,size);

        if(new != NULL)
        {
            @ptr = new;
        }

        return new == NULL;
    }


    // TODO: make this check if we can just extend the old block
    // rather than just allways requesting we get a new one...
    new := alloc_heap(heap,count,size);

    // failed to get a new buffer just back out
    // and leave the old one in place
    if(!new)
    {
        return true;
    }

    // copy over the old data to the new buffer
    node := cast(HeapNode@, old) - 1;

    // need to make sure we dont trash the buffer if this shrinks
    copy_size := min_u64((node.blocks - 1) * HEAP_BLOCK_SIZE,count * size);

    memcpy(cast(byte@,new),cast(byte@,old),copy_size);

    // free up the data
    free_heap(heap,ptr);

    // actually overwrite the old pointer
    @ptr = new;

    return false;
}

func realloc_heap_panic(heap : Heap@, ptr: byte@@, count: usize,size: usize)
{
    if(realloc_heap(heap,ptr,count,size))
    {
        crash_and_burn_oom("out of memory for heap realloc");
    }
}


func alloc_heap_panic(heap : Heap@, count : usize, size : usize) byte@
{
    ptr := alloc_heap(heap,count,size);

    if(!ptr)
    {
        crash_and_burn_oom("out of memory for heap alloc");
    }

    if(!check_heap_pointer_valid(heap,ptr))
    {
        std::exit(21);
    }

    // Pointer has been checked this is now safe to treat as a reference
    return cast(byte@,ptr);
}

func alloc_global_heap_panic(count : usize, size : usize) byte@
{
    return alloc_heap_panic(&itl_context.heap,count,size);
}

func realloc_global_heap_panic(ptr: byte@@, count: usize,size: usize)
{
    return realloc_heap_panic(&itl_context.heap,count,size);
}

func free_global_heap(ptr : byte@?)
{
    free_heap(&itl_context.heap,ptr);
}

func add_heap_block(heap : Heap@, node: HeapNode@)
{
    cur := heap.free_list;

    // Handle insertion at start
    if(!cur)
    {
        heap.free_list = node;
        return;
    }

    cur_reference := cast_ref(cur);

    // just insert at end
    while(cur_reference.next)
    {
        cur_reference = cast_ref(cur_reference.next);
    }

    cur_reference.next = node;
    node.prev = cur_reference;
}

func add_buffer(heap : Heap@, ptr : byte@, size : usize)
{
    if(!ptr)
    {
        return;
    }

    // NOTE: we need a block to actually store it
    blocks := (size / HEAP_BLOCK_SIZE);

    node := cast(HeapNode@, ptr);
    node.blocks = blocks;
    node.next = NULL;
    node.prev = NULL;
    node.free = true;

    node.canary_start = HEAP_CANARY_START;
    node.canary_end = HEAP_CANARY_END;

    // new memory added
    heap.capacity += blocks;

    if(node < heap.min)
    {
        heap.min = node;
    }

    end := node + node.blocks;

    if(end > heap.max)
    {
        heap.max = end;
    }

    // directly pass new block
    add_heap_block(heap,node); 

    verify_heap(heap); 
}

func make_heap(buf : byte[]) Heap
{
    heap : Heap;
    add_buffer(&heap,buf.data,buf.len);

    return heap;
}