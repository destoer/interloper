
struct HeapNode
{
    next : HeapNode@;
    blocks : u64;
}

struct Heap
{
    free_list : HeapNode@ = NULL;
}

constant HEAP_DEFAULT_BLOCK_COUNT : u32 = 1024;
constant HEAP_BLOCK_SIZE : u32 = HeapNode.size;
constant HEAP_DEFAULT_ALLOC_SIZE : u32 = HEAP_BLOCK_SIZE * HEAP_DEFAULT_BLOCK_COUNT;

func free_heap(heap : Heap@, ptr : byte@)
{
    if(ptr == NULL)
    {
        return;
    }

    insert := cast(HeapNode@,ptr) - 1;
    cur := heap.free_list;


    // handle insertion at start
    if(cur == NULL || insert < cur)
    {
        heap.free_list = insert;

        // attempt to collapse with old first
        if((insert + insert.blocks) == cur)
        {
            insert.blocks += cur.blocks;
            insert.next = cur.next;
        }

        else
        {
            insert.next = cur;
        }

        return;
    }


    // scan for correct insertion location
    // i.e insert > cur && insert < next
    while(cur.next != NULL && !(insert > cur && insert < cur.next))
    {
        cur = cur.next;
    }
    
    // at this point cur contains the lower bound
    // and cur.next contains the upper bound
    // insert is in the middle

    next := cur.next;

    // attempt to collapse upper
    if((insert + insert.blocks) == next)
    {
        insert.blocks += next.blocks;
        insert.next = next;
    }
    
    // cant collapse insert . next
    else
    {
        insert.next = cur.next;
    }

    // attempt to collapse lower
    if((cur + cur.blocks) == insert)
    {
        cur.blocks += insert.blocks;
        cur.next = insert.next;
    }

    // cant collapse cur . insert
    else
    {
        cur.next = insert;
    }
}

func request_buffer(heap : Heap@, size : u64) bool
{
    ptr := os_req_mem(size);

    if(ptr == cast(byte@,OS_INVALID_PTR))
    {
        return false;
    }

    add_buffer(heap,ptr,size);
    return true;
}

func alloc_heap(heap : Heap@, count : u32, size : u32) byte@
{
    bytes := count * size;

    // TODO: this overstimates block aligned data
    // we are just gonna ignore this for now
    req_blocks := (bytes / HEAP_BLOCK_SIZE) + 2;

    // attempt to find a block big enough for allocation?
    cur := heap.free_list;
    prev := cur;

    while(cur != NULL)
    {
        if(cur.blocks >= req_blocks)
        {
            // exact size just skip over
            if(cur.blocks == req_blocks)
            {
                prev.next = cur.next;
            }

            // fragment the block
            else
            {
                // cur block gets remaining
                remaining_blocks := cur.blocks - req_blocks;
                cur.blocks -= req_blocks;


                // new block contains the requested allocation
                cur += remaining_blocks;
                cur.blocks = req_blocks;
            }

            return cast(byte@,(cur + 1));
        }

        prev = cur;
        cur = cur.next;
    }

    req := max_u64(req_blocks * HEAP_BLOCK_SIZE,HEAP_DEFAULT_ALLOC_SIZE);

    // out of memory try get some more
    if(request_buffer(heap,req))
    {
        return alloc_heap(heap,count,size);
    }

    // cannot aquire any more memory we are out
    return NULL;
}

func alloc_heap_panic(heap : Heap@, count : u32, size : u32) byte@
{
    ptr := alloc_heap(heap,count,size);

    if(ptr == NULL)
    {
        crash_and_burn("out of memory for heap alloc: {}\n",size);
    }

    return ptr;
}

func add_buffer(heap : Heap@, ptr : byte@, size : u64)
{
    if(ptr == NULL)
    {
        return;
    }

    blocks := size / HEAP_BLOCK_SIZE;

    node := cast(HeapNode@,ptr);
    node.blocks = blocks;
    free_heap(heap,node + 1);  
}

func make_heap(buf : byte[]) Heap
{
    heap : Heap;
    add_buffer(&heap,buf.data,buf.len);

    return heap;
}