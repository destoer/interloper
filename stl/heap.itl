namespace std;

#attr(no_reorder)
struct HeapNode
{
    canary_start : u64;

    next : HeapNode@;

    // NOTE: inclusive Node
    blocks : u64;

    canary_end : u64;
}

struct Heap
{
    free_list : HeapNode@ = NULL;

    min: HeapNode@;
    max: HeapNode@;

    // NOTE: this is in block counts
    used : u64;
    capacity : u64;
}


constant HEAP_DEFAULT_BLOCK_COUNT : u32 = 1024;
constant HEAP_BLOCK_SIZE : u32 = HeapNode.size;
constant HEAP_DEFAULT_ALLOC_SIZE : u32 = HEAP_BLOCK_SIZE * HEAP_DEFAULT_BLOCK_COUNT;

constant HEAP_CANARY_START : u64 = 0xCAFE_DEAD_C0DE_BABE;
constant HEAP_CANARY_END : u64 = 0xFACE_DEAD_FEED_CAFE;

func usable_blocks(heap: const Heap@) u64
{
    return (heap.capacity - heap.used);
}

func heap_usage(heap: const Heap@)
{
    print("heap usage:\n");
    print("used: {}\n",heap.used * HEAP_BLOCK_SIZE);
    print("capacity: {}\n",heap.capacity * HEAP_BLOCK_SIZE);
    print("free: {}\n",usable_blocks(heap) * HEAP_BLOCK_SIZE);

    putchar('\n');
}

func print_heap(heap: const Heap@)
{
    cur := heap.free_list;

    if(!cur)
    {
        return;
    }

    // TODO: need to work on rtti so we dont need casts for this
    while(cur)
    {
        print("node {} : {x} -> {}\n",cast(byte@,cur),cur.blocks,cast(byte@,cur.next));
        cur = cur.next;
    }
}

func verify_node(heap: const Heap@,node: const HeapNode@)
{
    if(!node)
    {
        return;
    }

    if(node > heap.max || node < heap.min)
    {
        // TODO: need to work on rtti so we dont need casts for this
        crash_and_burn("[HEAP]: node {} is out of range: {} : {}",
            cast(byte@,node),cast(byte@,heap.min),cast(byte@,heap.max));
    }

    if(node.canary_start != HEAP_CANARY_START || node.canary_end != HEAP_CANARY_END) 
    {
        crash_and_burn("[HEAP]: node {} : {x} canary trashed {x} : {x} -> {x} : {x}",
            cast(byte@,node),node.blocks,HEAP_CANARY_START,HEAP_CANARY_END,node.canary_start,node.canary_end);
    }
}

func verify_heap(heap: const Heap@)
{
    cur := heap.free_list;

    if(!cur)
    {
        return;
    }

    while(cur)
    {
        verify_node(heap,cur);
        cur = cur.next;
    }
}

// internal function to add node back to free list
// NOTE: null checking must be done by higher level functions
func free_heap_node(heap: Heap@, insert: HeapNode@)
{
    verify_node(heap,insert);
    verify_heap(heap);

    cur := heap.free_list;

    // handle insertion at start
    if(!cur || insert < cur)
    {
        heap.free_list = insert;

        // attempt to collapse with old first
        if((insert + insert.blocks) == cur)
        {
            insert.blocks += cur.blocks;
            insert.next = cur.next;
        }

        else
        {
            insert.next = cur;
        }

        return;
    }


    // scan for correct insertion location
    while(cur.next != NULL && insert > cur.next)
    {
        cur = cur.next;
    }
    
    // at this point cur contains the lower bound
    // and cur.next contains the upper bound
    // insert is in the middle

    next := cur.next;

    // attempt to collapse upper
    if((insert + insert.blocks) == next)
    {
        insert.blocks += next.blocks;
        insert.next = next.next;
    }
    
    // cant collapse insert.next = next
    else
    {
        insert.next = next;
    }

    // attempt to collapse lower
    if((cur + cur.blocks) == insert)
    {
        cur.blocks += insert.blocks;
        cur.next = insert.next;
    }

    // cant collapse cur.next = insert
    else
    {
        cur.next = insert;
    }
}



func free_heap(heap : Heap@, ptr : byte@@)
{
    if(!@ptr)
    {
        return;
    }

    // block info is behind the buffer
    insert := cast(HeapNode@,@ptr) - 1;

    // set our ptr to null
    @ptr = NULL;

    // track memory reclaim
    heap.used -= insert.blocks;

    // actually add it back to the free list
    free_heap_node(heap,insert);
}

func request_buffer(heap : Heap@, size : usize) bool
{
    ptr := os_req_mem(size);

    if(ptr == cast(byte@,OS_INVALID_PTR))
    {
        return false;
    }

    add_buffer(heap,ptr,size);
    return true;
}

func alloc_heap(heap : Heap@, count : usize, size : usize) byte@
{
    verify_heap(heap);

    bytes := count * size;

    req_blocks := ((bytes + HEAP_BLOCK_SIZE - 1) / HEAP_BLOCK_SIZE) + 1;

    // attempt to find a block big enough for allocation?
    cur := heap.free_list;
    prev := cur;

    while(cur)
    {
        if(cur.blocks >= req_blocks)
        {
            // exact size just skip over
            if(cur.blocks == req_blocks)
            {
                // we are returning start
                // update the tracking
                if(cur == heap.free_list)
                {
                    heap.free_list = cur.next;
                }
                
                else
                {
                    prev.next = cur.next;
                }

                // cleave old link
                cur.next = NULL;
            }

            // fragment the block
            else
            {
                // cur block gets remaining
                cur.blocks -= req_blocks;

                // new block contains the requested allocation
                cur += cur.blocks;


                // setup the new block
                cur.blocks = req_blocks;
                cur.next = NULL; 
            }
            
            cur.canary_start = HEAP_CANARY_START;
            cur.canary_end = HEAP_CANARY_END;

            // mark heap usage
            heap.used += cur.blocks;

            // data is after buffer
            return cast(byte@,cur + 1);
        }

        prev = cur;
        cur = cur.next;
    }

    req := max_u64(req_blocks * HEAP_BLOCK_SIZE,HEAP_DEFAULT_ALLOC_SIZE);

    // out of memory try get some more
    if(request_buffer(heap,req))
    {
        return alloc_heap(heap,count,size);
    }

    // cannot aquire any more memory we are out
    return NULL;
}

func realloc_heap(heap : Heap@, ptr: byte@@, count: usize,size: usize) bool
{
    // get the actual data pointer
    old := @ptr;

    // first alloc just behave like alloc
    if(!old)
    {
        @ptr = alloc_heap(heap,count,size);
        return @ptr == NULL;
    }


    // TODO: make this check if we can just extend the old block
    // rather than just allways requesting we get a new one...
    new := alloc_heap(heap,count,size);


    // failed to get a new buffer just back out
    // and leave the old one in place
    if(!new)
    {
        return true;
    }

    // copy over the old data to the new buffer
    node := cast(HeapNode@,old) - 1;

    // need to make sure we dont trash the buffer if this shrinks
    copy_size := min_u64((node.blocks - 1) * HEAP_BLOCK_SIZE,count * size);

    memcpy(new,old,copy_size);

    // free up the data
    free_heap(heap,ptr);

    // actually overwrite the old pointer
    @ptr = new;

    return false;
}

func realloc_heap_panic(heap : Heap@, ptr: byte@@, count: usize,size: usize)
{
    if(realloc_heap(heap,ptr,count,size))
    {
        crash_and_burn("out of memory for heap realloc: {} : {}\n",count,size);
    }
}


func alloc_heap_panic(heap : Heap@, count : usize, size : usize) byte@
{
    ptr := alloc_heap(heap,count,size);

    if(!ptr)
    {
        crash_and_burn("out of memory for heap alloc: {} : {}\n",count,size);
    }

    return ptr;
}

func add_buffer(heap : Heap@, ptr : byte@, size : usize)
{
    if(!ptr)
    {
        return;
    }

    // NOTE: we need a block to actually store it
    blocks := (size / HEAP_BLOCK_SIZE);

    node := cast(HeapNode@,ptr);
    node.blocks = blocks;
    node.next = NULL;

    node.canary_start = HEAP_CANARY_START;
    node.canary_end = HEAP_CANARY_END;

    // new memory added
    heap.capacity += blocks;

    // dignostic checking
    if(node + blocks > heap.max)
    {
        heap.max = node + blocks;
    }

    if(node < heap.min)
    {
        heap.min = node;
    }

    // directly pass new block
    free_heap_node(heap,node);  
}

func make_heap(buf : byte[]) Heap
{
    heap : Heap;
    add_buffer(&heap,buf.data,buf.len);

    return heap;
}